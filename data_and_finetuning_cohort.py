# -*- coding: utf-8 -*-
"""data_and_finetuning_cohort.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qaygyTErbMPgvkFHuHpNYAV9GQQCPlYi

# **PROJET DUDA COHORTSCORE**

---

# 1 - Partie Analyse exploratoire

Setup
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""Import du dataset contenant au minimum 200 000 lignes, avec une diversité de données numériques, catégorielles et temporelles"""

# Configurer l’API Kaggle
!mkdir -p ~/.kaggle
!cp /content/drive/MyDrive/credentials/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# Télécharger & dézipper les jeux Kaggle
!kaggle datasets download -d yasserh/instacart-online-grocery-basket-analysis-dataset \
    -p /content/drive/MyDrive/data/raw --unzip

base_path = "/content/drive/MyDrive/data/raw/instacart-online-grocery-basket-analysis-dataset"

orders = pd.read_csv(f"{base_path}/orders.csv")
products = pd.read_csv(f"{base_path}/products_with_prices.csv")
aisles = pd.read_csv(f"{base_path}/aisles.csv")
order_products_prior = pd.read_csv(f"{base_path}/order_products__prior.csv")
order_products_train = pd.read_csv(f"{base_path}/order_products__train.csv")

"""Decouverte exploratoire des données

orders.csv — consists of order details placed by any user — shape: (3421083, 7)


*   Order_id : Unique for every order
*   User_id : Unique for every user
*   Eval_set : ( prior / train / test)
*   Order_number : ith order placed by user
*   Order_dow : Day of week
*   Order_hour_of_day : Time of day in hr
*   Days_since_prior_order : difference in days between 2 orders
"""

print("ORDERS:")
display(orders.head())
print(orders.shape)

print("PRODUCTS:")
display(products.head())
print(products.shape)

print("AISLES:")
display(aisles.head())
print(aisles.shape)

"""Merge"""

order_products_all = pd.concat([order_products_prior, order_products_train], ignore_index=True)
order_products_all.sort_values(by=["order_id", "add_to_cart_order"], inplace=True)
print("OPA:")
display(order_products_all.head())
print(order_products_all.shape)

"""Nettoyage des données"""

nan_counts = orders.isna().sum()
print("NaN orders :\n", nan_counts)

dup_total = orders.duplicated().sum()
print(f"Nombre de lignes dupliquées orders : {dup_total}")

nan_counts2 = products.isna().sum()
print("NaN products :\n", nan_counts2)

dup_total2 = products.duplicated().sum()
print(f"Nombre de lignes dupliquées products : {dup_total2}")

nan_counts3 = aisles.isna().sum()
print("NaN aisles :\n", nan_counts3)

dup_total3 = aisles.duplicated().sum()
print(f"Nombre de lignes dupliquées aisles : {dup_total3}")

nan_counts4 = order_products_all.isna().sum()
print("NaN opa :\n", nan_counts4)

dup_total4 = order_products_all.duplicated().sum()
print(f"Nombre de lignes dupliquées opa : {dup_total4}")

# Gérer les NaN de la 1ʳᵉ commande → 0 jour avant
orders["days_since_prior_order"] = orders["days_since_prior_order"].fillna(1)

"""Création des variables supplémentaires & 7 analyses"""

## 2 Best sellers aisles & products (with % of repeat)

# Merge des produits avec les infos produits (nom, aisle, etc.)
merged = order_products_all.merge(products, on='product_id', how='left')

# Merge avec la table des rayons (aisles)
merged = merged.merge(aisles, on='aisle_id', how='left')

display(merged.head(20))
print(merged.shape)

# Nettoyage de la colonne prix
merged["price"] = merged["price"].replace('[€]', '', regex=True).astype(float)
display(merged.head(20))

distinct_orders = merged['order_id'].nunique()
print(f"Nombre de commandes distinctes : {distinct_orders}")

# 2) Top 12 produits par quantité et CA
top_qty_prod = (
    merged
    .groupby('product_name')
    .size()
    .reset_index(name='total_qty')
    .nlargest(12, 'total_qty')
)
top_rev_prod = (
    merged
    .groupby('product_name')['price']
    .sum()
    .reset_index(name='total_rev')
    .nlargest(10, 'total_rev')
)

# 3) Union des produits
prods = pd.Index(top_qty_prod['product_name']).union(top_rev_prod['product_name'])

# 4) Recalcul des métriques sur cet ensemble
df_prod = (
    merged
    .groupby('product_name')
    .agg(
        total_qty=('order_id', 'count'),
        total_rev=('price',   'sum')
    )
    .loc[prods]
    .sort_values('total_qty', ascending=True)
)

# 5) Figure en deux panneaux
fig, (ax1, ax2) = plt.subplots(
    1, 2,
    figsize=(20, 10),
    gridspec_kw={'width_ratios': [2, 1]}
)

y = list(range(len(df_prod)))

# — Panel 1 : barres dual-axe Quantité vs CA
bars1 = ax1.barh(
    [i - 0.2 for i in y],
    df_prod['total_qty'],
    height=0.4,
    color='steelblue',
    label='Quantité vendue'
)
ax1.set_yticks(y)
ax1.set_yticklabels(df_prod.index, fontsize=9)
ax1.set_xlabel('Quantité vendue')

ax1_tw = ax1.twiny()
bars2 = ax1_tw.barh(
    [i + 0.2 for i in y],
    df_prod['total_rev'],
    height=0.4,
    color='darkorange',
    label="Chiffre d'affaires (€)"
)
ax1_tw.set_xlabel("Chiffre d'affaires (€)")

# Légende combinée
handles = [bars1, bars2]
labels  = [h.get_label() for h in handles]
ax1.legend(handles, labels, loc='lower right')

ax1.set_title("Top produits : Quantité vs Chiffre d'affaires")

# — Panel 2 : camembert du CA

ordered = df_prod['total_rev'].sort_values(ascending=False)
ax2.pie(
    ordered.values,
    autopct='%1.1f%%',
    labels=ordered.index,
    startangle=90,
    wedgeprops={'edgecolor': 'white'}
)


ax2.axis('equal')
ax2.set_title("Répartition du CA par produit\n(Top produits)")

plt.tight_layout()
plt.show()

# 2) Top 12 par quantité et par CA
top_qty_aisle = (
    merged
    .groupby('aisle')
    .size()
    .reset_index(name='total_qty')
    .nlargest(12, 'total_qty')
)
top_rev_aisle = (
    merged
    .groupby('aisle')['price']
    .sum()
    .reset_index(name='total_rev')
    .nlargest(10, 'total_rev')
)

# 3) Union des rayons et recalcul des métriques
aisles = pd.Index(top_qty_aisle['aisle']).union(top_rev_aisle['aisle'])
df_aisle = (
    merged
    .groupby('aisle')
    .agg(
        total_qty=('order_id', 'count'),
        total_rev=('price',   'sum')
    )
    .loc[aisles]
    .sort_values('total_qty', ascending=True)
)

# 4) Figure à deux panneaux
fig, (ax1, ax2) = plt.subplots(
    1, 2,
    figsize=(18, 8),
    gridspec_kw={'width_ratios': [2, 1]}
)

y = list(range(len(df_aisle)))

# — Panel 1 : dual-axe Quantité vs CA
bars1 = ax1.barh(
    [i - 0.2 for i in y],
    df_aisle['total_qty'],
    height=0.4,
    color='steelblue',
    label='Quantité vendue'
)
ax1.set_yticks(y)
ax1.set_yticklabels(df_aisle.index)
ax1.set_xlabel('Quantité vendue')

ax2b = ax1.twiny()
bars2 = ax2b.barh(
    [i + 0.2 for i in y],
    df_aisle['total_rev'],
    height=0.4,
    color='darkorange',
    label="Chiffre d'affaires (€)"
)
ax2b.set_xlabel("Chiffre d'affaires (€)")

handles = [bars1, bars2]
labels  = [h.get_label() for h in handles]
ax1.legend(handles, labels, loc='lower right')

ax1.set_title("Top rayons : Quantité vs CA")

# — Panel 2 : pie chart du CA
orderedaisles = df_aisle['total_rev'].sort_values(ascending=False)
ax2.pie(
    orderedaisles.values,
    labels=orderedaisles.index,
    autopct='%1.1f%%',
    startangle=90,
    wedgeprops={'edgecolor': 'white'}
)
ax2.axis('equal')  # pour un cercle parfait
ax2.set_title("Répartition du CA par rayon\n(Top aisles)")

plt.tight_layout()
plt.show()

# Fusion de orders avec order_products_all
full_data = merged.merge(orders, on='order_id', how='left')

# 2 Best moments (day and hour)

top_hours = full_data.groupby('order_hour_of_day')['product_id'].count()
top_days = full_data.groupby('order_dow')['product_id'].count()


# Matrice Jour x Heure (nombre de produits commandés)
sales_matrix = (
    full_data
    .groupby(["order_dow", "order_hour_of_day"])
    .size()
    .unstack(fill_value=0)
)

day_names = ['Sun','Mon','Tue','Wed','Thu','Fri','Sat']
sales_matrix.index = day_names

# Remplacer l’index de top_days par les noms
top_days.index = [day_names[d] for d in top_days.index]

sales_thousands = sales_matrix / 1000

#LTV

total_price_per_customer = (
    full_data
    .groupby('user_id')['price']        # on cible la colonne price
    .sum()                                # calcul du total
    .reset_index(name='total_price')      # remet order_id en colonne et renomme
)
print(total_price_per_customer)
print(total_price_per_customer.describe())

from matplotlib.colors import LinearSegmentedColormap

# Création de la figure avec 3 sous-graphes
fig, axes = plt.subplots(
    1, 3,
    figsize=(20, 6),
    gridspec_kw={'width_ratios': [1, 1, 1.2]}
)

# 1) Histogramme : produits commandés par jour
sns.barplot(
    x=top_days.index,
    y=top_days.values,
    palette='Greens',
    ax=axes[0]
)
axes[0].set_title("Produits commandés par jour")
axes[0].set_xlabel("Jour de la semaine")
axes[0].set_ylabel("Nombre de produits")
axes[0].tick_params(axis='x', rotation=45)

# 2) Histogramme : produits commandés par heure
sns.barplot(
    x=top_hours.index,
    y=top_hours.values,
    palette='Greens',
    ax=axes[1]
)
axes[1].set_title("Produits commandés par heure")
axes[1].set_xlabel("Heure de la journée")
axes[1].set_ylabel("Nombre de produits")
axes[1].tick_params(axis='x', rotation=45)

# 3) Heatmap : commandes jour x heure en milliers
cmap_wg = LinearSegmentedColormap.from_list('RedGreen', ['white','green'])
sns.heatmap(
    sales_thousands,
    cmap=cmap_wg,
    annot=True,
    fmt='.0f',
    annot_kws={'fontsize':8},
    cbar_kws={'label': 'Commandes (en k)'},
    linewidths=0.5,
    linecolor='white',
    ax=axes[2]
)
axes[2].set_title("Commandes par jour et heure (en milliers)")
axes[2].set_xlabel("Heure de la journée")
axes[2].set_ylabel("Jour de la semaine")
axes[2].tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

# 3 RFM Cluster

# 1. Copie de full_data
df = full_data.copy()

# 2. Gérer les NaN de la 1ʳᵉ commande → 0 jour avant
df["days_since_prior_order"] = df["days_since_prior_order"].fillna(0)

# 3. Ordonner pour chaque utilisateur
df = df.sort_values(["user_id", "order_number"])

# 5. Calculer Recency (Récence de la dernière commande)
recency = (
    df
    .groupby("user_id")["days_since_prior_order"]
    .nunique()
    .reset_index(name="Recency")
)

# 6. Calculer Frequency (nombre total de commandes)
frequency = (
    df
    .groupby("user_id")["order_number"]
    .nunique()
    .reset_index(name="Frequency")
)

# 7. Calculer Monetary (€ total dépensé)
#    (adaptez si vous avez un champ `price * quantity`)
monetary = (
    df
    .groupby("user_id")["price"]
    .sum()
    .reset_index(name="Monetary")
)

# 8. Merge final RFM
rfm = recency.merge(frequency, on="user_id") \
               .merge(monetary, on="user_id")

# 9. Aperçu
print(rfm.head())
print(rfm.info())
print(rfm.describe())

# CLUSTERING

from sklearn.preprocessing import StandardScaler

# Choix : soit RFM bruts, soit scores (plus simple à interpréter)
X = rfm[["Recency", "Frequency", "Monetary"]].copy()

# Standardisation
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia = []
K_range = range(1, 11)

for k in K_range:
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(X_scaled)
    inertia.append(model.inertia_)

# Courbe du coude
plt.figure(figsize=(8, 4))
plt.plot(K_range, inertia, marker='o')
plt.title("Méthode du coude – Choix de K")
plt.xlabel("Nombre de clusters")
plt.ylabel("Inertie (distorsion)")
plt.xticks(K_range)
plt.grid(True)
plt.show()

# Choix du K où la courbe commence à "plier" : souvent entre 3 et 6.
k = 4
kmeans = KMeans(n_clusters=k, random_state=42)
rfm["Cluster"] = kmeans.fit_predict(X_scaled)

# Moyenne des variables RFM par cluster
cluster_summary = (
    rfm.groupby("Cluster")[["Recency", "Frequency", "Monetary"]]
    .median()
    .round(1)
    .sort_values("Monetary", ascending=False)
)

# 1. Calculer le nombre de clients par cluster
cluster_counts = rfm.groupby("Cluster").size().rename("Count")

# 2. Joindre ce décompte à votre summary existant
cluster_summary_with_count = (
    cluster_summary
    .join(cluster_counts)                   # ajoute la colonne Count
    .sort_values("Monetary", ascending=False)  # tri facultatif
)

# 3. Afficher le résultat
display(cluster_summary_with_count)

# 1. Tracez d’abord votre pairplot
g = sns.pairplot(
    rfm,
    vars=["Recency", "Frequency", "Monetary"],
    hue="Cluster",
    palette="Set2"
)

# 2. Dictionnaire de mapping (int → str)
label_dict = {
    0: "Occasionnels",
    1: "Champions",
    2: "Noob",
    3: "Risque de churn"
}

# 3. Remplacez les textes de légende
for text in g._legend.texts:
    orig = text.get_text()
    # vérifie si c’est un chiffre, puis remplace
    if orig.isdigit():
        text.set_text(label_dict[int(orig)])

plt.suptitle("Clusters RFM (labels)", y=1.02)
plt.show()

from mpl_toolkits.mplot3d import Axes3D
import plotly.express as px
import plotly.graph_objects as go

# Nombre de clusters (ici 6)
n_clusters = rfm["Cluster"].nunique()

# Récupération de la liste de couleurs Set2
palette = sns.color_palette("Set2", n_clusters)

# Si vos labels de cluster sont 0,1,2,3,4,5 (sinon trier ou renuméroter)
cluster_labels = sorted(rfm["Cluster"].unique())
color_dict = dict(zip(cluster_labels, palette))

# ––––––––––––––––––––––––––––––––––––––––––––––––––
fig = plt.figure(figsize=(10, 8))
ax  = fig.add_subplot(111, projection='3d')
colors = rfm["Cluster"].map(color_dict)


sc = ax.scatter(
    rfm["Recency"],
    rfm["Frequency"],
    rfm["Monetary"],
    c=colors,
    cmap="Set2",       # palette discrète
    s=40,              # taille des points
    alpha=0.7          # transparence
)

ax.set_xlabel("Recency")
ax.set_ylabel("Frequency")
ax.set_zlabel("Monetary")
ax.set_title("Clusters RFM en 3D")

# Légende automatique des clusters
legend1 = ax.legend(
    *sc.legend_elements(),
    title="Cluster"
)
ax.add_artist(legend1)

plt.show()

from matplotlib.colors import rgb2hex

hex_palette = [rgb2hex(color) for color in palette]

fig = px.scatter_3d(
    rfm,
    x="Recency", y="Frequency", z="Monetary",
    color="Cluster",
    color_discrete_sequence=hex_palette,
    size_max=6,
    opacity=0.7,
    title="Clusters RFM Interactif (Set2)"
)
fig.update_layout(margin=dict(l=0, r=0, b=0, t=30))
fig.show()

"""Export"""

# 2. Importer os pour créer le dossier si nécessaire
import os

# 1) Construire la table finale
final_df = full_data.merge(
    rfm[['user_id', 'Recency', 'Frequency', 'Monetary', 'Cluster']],
    on='user_id',
    how='left'
)

drive_dir = '/content/drive/MyDrive/data/raw'

output_path = os.path.join(drive_dir, 'dataset_final_ok.parquet')
final_df.to_parquet(output_path, index=False)

print(f"P nettoyé exporté vers : {output_path}")

print(final_df.shape)

df_sample1million = final_df.head(n=1000000)

df_sample2million = final_df.head(n=2000000)

# Calculer le montant total dépensé par chaque utilisateur sur l'ensemble des données
total_price_per_user_final = final_df.groupby('user_id')['price'].sum().reset_index()

total_price_per_user_1m = df_sample1million.groupby('user_id')['price'].sum().reset_index()

total_price_per_user_2m = df_sample2million.groupby('user_id')['price'].sum().reset_index()


# Renommer la colonne pour plus de clarté
total_price_per_user_final.rename(columns={'price': 'total_user_price'}, inplace=True)

total_price_per_user_1m.rename(columns={'price': 'total_user_price'}, inplace=True)

total_price_per_user_2m.rename(columns={'price': 'total_user_price'}, inplace=True)


# Afficher les 5 premières lignes du résultat
print("\nPrix total par utilisateur (sur l'ensemble des données) :")
display(total_price_per_user_final)
total_price_per_user_final.describe()
print(total_price_per_user_final.median())
print(total_price_per_user_final.mean())

print(total_price_per_user_1m.median())
print(total_price_per_user_1m.mean())

print(total_price_per_user_2m.median())
print(total_price_per_user_2m.mean())

output_path1m = os.path.join(drive_dir, 'data_sample_1million.parquet')
df_sample1million.to_parquet(output_path1m, index=False)

print(f"Parquet nettoyé exporté vers : {output_path1m}")

output_path2m = os.path.join(drive_dir, 'data_sample_2million.parquet')
df_sample2million.to_parquet(output_path2m, index=False)

print(f"Parquet nettoyé exporté vers : {output_path2m}")

"""Visualisation avec filtres dynamiques sur l'app.py streamlit"""



"""# 2 - Partie text mining"""

import re
from wordcloud import WordCloud, STOPWORDS

# 1. Charger le texte de l’article
text = """
RFM Segmentation: What Is It, and How Can Marketers Use It?
RFM Segmentation: What Is It, and How Can Marketers Use It?
By Ethan Shust, Last Updated: June 2, 2023

Audience segmentation is a powerful way to analyze your customer data, define your target customer persona(s), and tailor your acquisition/retention efforts to each unique persona for the best-performing results. RFM Segmentation is a method of segmenting your customer audiences by grouping them based on three key vectors: recency, frequency, and monetary value. Let’s dig in!

The Foundation: Marketing Segmentation 101
Before we dive into the depths of RFM Segmentation (Recency, Frequency, Monetary Value), let's take a step back and review what segmentation means for marketers.

Segmentation is a crucial component of every team's marketing strategy. It involves dividing a larger market into smaller groups of consumers with similar characteristics. By doing this, businesses can identify different customer groups and develop targeted marketing strategies that cater to each group's specific wants and needs. When done correctly, this leads to higher engagement, conversion rates, and customer loyalty.

Marketers often use the following methods to segment their customers:

Demographic Segmentation: This method groups customers based on demographic factors, such as age, gender, income, education, occupation, and family status.
Geographic Segmentation: This method groups customers based on their location, such as country, region, city, or neighborhood.
Psychographic Segmentation: This method groups customers based on personality traits, values, beliefs, interests, and lifestyles.
Behavioral Segmentation: This method groups customers based on their behaviors, such as buying habits, product usage, brand loyalty, and decision-making processes.
For more information on each type of segmentation, check out this awesome article from Yieldify.


From the above list, behavioral segmentation is the method that informs RFM audiences, as these audiences are developed based on customers' past purchase behavior.

But before we dive into RFM audiences, here’s a short list of the ways that analyzing transactional data benefits your business:

Clear, Objective Criteria: Transactional data uncovers the clear, objective criteria critical for identifying and targeting high-value customers.
Accurate and Reliable Data: Transactional data is typically more accurate and reliable than other forms of data, such as survey data or demographic data. Other forms of data can be subject to self-reporting biases and general errors.
More Actionable Insights: Transactional data provides more actionable insights into customer behavior and preferences, allowing businesses to develop targeted and effective marketing strategies.
Immediate Feedback: Transactional data provides immediate feedback on the effectiveness of marketing campaigns and promotions. This allows businesses to make quick optimizations necessary for improving results.
So now that you’re sold on why transactional data matters, let’s talk RFM.


The RFM model
RFM segmentation is a customer segmentation technique that categorizes customers based on three vectors of their past purchasing behaviors: Recency, Frequency, and Monetary Value. We’ve unpacked each of these vectors for you below.

Recency: This refers to how much time has elapsed since a customer last made a purchase. In most cases, customers who have purchased more recently are valuable customers likely to engage with the brand again.
Frequency: This refers to how often a customer makes purchases. Customers who make frequent purchases are typically a brand’s most valuable customers, demonstrating higher brand loyalty than those who purchase less often.
Monetary Value: This refers to the amount of money a customer has spent on purchases over a given period of time. Of course, customers who spend a comparatively high amount of money are valuable customers with higher brand loyalty, and are most likely to purchase again. Customers with high monetary value are typically the most valuable customers for a given brand.
Using the RFM segmentation method, customers are ranked based on each of these three vectors and then divided - or segmented - into groups based on their scores.

For example, a customer who purchased within the past few days, purchases your product at least once a month, and spends a comparatively high amount of money with your business will be segmented into a high-value segment.

Conversely, a customer who hasn’t purchased from you in a year with only two low-value historical purchases would be segmented into a low-value segment.

‍

Common RFM Segments
Here are some of the most popular segments brands develop using the RFM model:

High-Value Customers: These are customers who score high on all three metrics (Recency, Frequency, and Monetary Value). They are typically the most engaged and loyal customers, likely to make frequent, high-value purchases. Businesses can target these customers with exclusive offers, loyalty programs, and personalized marketing campaigns to keep them engaged and encourage them to continue buying from the brand in the future.
New Customers: These are customers who have made a recent purchase (high score for Recency), but have not yet established a high level of loyalty or spending (low scores for Frequency and Monetary Value). Businesses can target these customers with welcome offers and promotions to encourage them to make another purchase and/or establish a stronger personal relationship with the brand.
High Risk Customers: These are customers who have not made a purchase recently (low score for Recency), but have previously made frequent and high-value purchases (high scores for Frequency and Monetary Value). These customers are segmented to flag them as they likely need some sort of nurturing to get them to return to the brand. Businesses can use this segment to launch re-engagement campaigns, personalized offers, and loyalty programs to encourage future purchases.
Low-Value Customers: These are customers who score low on all three metrics (Recency, Frequency, and Monetary Value). They are typically the least engaged and loyal customers and are unlikely to make frequent or high-value purchases. Businesses can target these customers with promotions and special offers to encourage them to make another purchase, or they can be excluded from marketing campaigns to focus on higher-value customers.
‍

Building an RFM analysis
Now that you know the what and the why, it’s time to dig into the how-to.

PS: Triple Whale offers pre-built, plug & play RFM segments for your analysis if you’re ready to get rocking and rolling. If you’re feeling more manual, check out the steps below.

Gather transactional data from your customer database or CRM system. This data should include purchase histories such as date, frequency, and transaction value for each customer.
Calculate RFM scores by assigning a score for each of the three metrics (Recency, Frequency, and Monetary Value) based on the customer's past purchase behavior. For instance, a recent purchase earns a high Recency score, while frequent purchases result in a high frequency score.
After calculating RFM scores for each customer, you can segment customers into groups based on their scores. One common approach is to use quartiles to divide customers into four equal groups based on their scores for each metric.
After segmenting customers, analyzing their characteristics and behavior can reveal patterns and trends. This can assist in improving marketing and customer engagement strategies.
Develop marketing strategies for each customer segment. High-value customers can be targeted with exclusive offers and loyalty programs, while low-value customers can be targeted with promotions to encourage repeat purchases.
Regularly monitor and refine your marketing strategies and RFM segmentation based on customer behavior to improve customer engagement and maximize customer value.
This is admittedly a very time-consuming process…so if you’re looking for a faster path to insights, keep reading to learn how Triple Whale unlocks ready-to-go RFM Analysis.

‍

Triple Whale’s SCDP and RFM audiences
For Shopify-based brands, Triple Whale’s Smart Customer Data Platform comes pre-loaded with 6 powerful RFM segments. These are Triple Whale’s own AI-generated RFM audiences.

To build these audiences, Triple Whale examines all of your customers’ historical data and splits them into buckets based on RFM scoring. In Triple Whale’s SCDP, these segments are highlighted with a square AI icon.

Triple Whale’s RFM audiences are defined as follows:

Loyal = Customers who buy the most often from your store.
Core = Highly engaged customers who have bought most recently, the most often, and have generated the most revenue.
Newbies = First-time buyers on your site.
Whales = Customers who have generated the most revenue for your store.
Promising = Customers who return often, but do not spend a lot.
Lost = Customers who have made one purchase but have not been known to return.
Daniel Okon, Founder/CEO of ACTIV Agency, recently tweeted about these RFM audiences in action:


Daniel is the Founder/CEO of ACTIV, an agency that helps 7 and 8-figure brands unlock the key components that help them scale profitably. He also has a strong Twitter game. Check him out!

‍

The RFM Model for Marketers
For marketers, leveraging the RFM Model unlocks detailed insights into customer behavior and preferences based directly on transactional data. With these insights, brands can assess customer engagement and loyalty levels, identify high loss-risk customers, and strategically tailor their marketing strategies to each unique persona. Additionally, RFM Analysis provides businesses with the data necessary to determine which channels, campaigns, and marketing tactics result in the highest return on investment (ROI).

In today’s crazy, competitive market, RFM segmentation & analysis is an extremely powerful tool for achieving every brand’s main goal: making more money to grow the business.

Ready to start digging in on RFM? Check out Triple Whale’s Smart Customer Data Platform and start exploring your brand’s smart RFM audiences today.

Book A Free Demo Today!"""

# 2. Nettoyage du texte
#   - On supprime tous les caractères non alphabétiques (ponctuation, chiffres, etc.)
#   - On convertit en minuscules pour uniformiser
text_clean = re.sub(r'[^A-Za-z\s]', '', text).lower()

# 3. Tokenisation
#   - On découpe le texte en mots (tokens) sur les espaces
tokens = text_clean.split()

# 4. Suppression des stopwords
#   - Les stopwords sont des mots très fréquents (articles, prépositions…)
#     qui n’apportent pas de sens particulier à l’analyse
stopwords = set(STOPWORDS)
tokens_filtered = [t for t in tokens if t not in stopwords]

# 5. Génération du nuage de mots
processed_text = " ".join(tokens_filtered)
wc = WordCloud(width=800, height=400).generate(processed_text)

# 6. Affichage (avec matplotlib)
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 7.5))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()
